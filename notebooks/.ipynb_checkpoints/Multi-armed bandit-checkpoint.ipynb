{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandit problem\n",
    "\n",
    "### Toy policy gradient method to solve multi armed bandit problem\n",
    "\n",
    "* The value here gives the probability with which the bandit gives a positive reward (when the person earns)\n",
    "* Initially assign equal probabilities to each of the bandits\n",
    "* Epsilon is the exploration probability\n",
    "* Moving average baseline is used\n",
    "\n",
    "### References\n",
    "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.25, 0.25, 0.25]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits = [0.1,0.4,0.9,0.7] # we will use this as p in a bernouli distribution to get output of each arm\n",
    "num_bandits = len(bandits)\n",
    "weights = [1/num_bandits]*num_bandits\n",
    "epsilon = 0.5 # Random action with 50% probability\n",
    "num_episodes = 100\n",
    "test_episodes = 10\n",
    "baseline = 0\n",
    "alpha = 0.8\n",
    "lr = 0.05\n",
    "running_reward = [0]*num_bandits\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the agent is able to pick up that third bandit is giving better rewards on an average. So far so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Chosen arm: 3, Running rewards: [0, 0, 0, 1]\n",
      "Iter: 10, Chosen arm: 2, Running rewards: [0, 0, 3, 2]\n",
      "Iter: 20, Chosen arm: 2, Running rewards: [0, -1, 9, 1]\n",
      "Iter: 30, Chosen arm: 0, Running rewards: [-3, -3, 11, 4]\n",
      "Iter: 40, Chosen arm: 2, Running rewards: [-4, -4, 17, 4]\n",
      "Iter: 50, Chosen arm: 3, Running rewards: [-4, -5, 19, 7]\n",
      "Iter: 60, Chosen arm: 3, Running rewards: [-5, -4, 24, 8]\n",
      "Iter: 70, Chosen arm: 3, Running rewards: [-8, -5, 29, 9]\n",
      "Iter: 80, Chosen arm: 2, Running rewards: [-8, -7, 33, 11]\n",
      "Iter: 90, Chosen arm: 2, Running rewards: [-8, -8, 37, 12]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    if np.random.uniform(low = 0.0, high = 1.0) < epsilon:\n",
    "        arm = np.random.randint(low = 0, high = len(bandits))\n",
    "    else:\n",
    "        arm = np.argmax(weights)\n",
    "    reward = np.random.binomial(1, bandits[arm])\n",
    "    if reward == 0:\n",
    "        reward = -1\n",
    "    running_reward[arm] += reward\n",
    "    advantage = reward - baseline\n",
    "    weights[arm] = weights[arm] + lr*advantage\n",
    "    baseline = baseline*alpha + (1 - alpha)*reward\n",
    "    if i%10 == 0:\n",
    "        print(\"Iter: {}, Chosen arm: {}, Running rewards: {}\".format(i, arm, running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits = [0.1,0.4,0.9,0.7,0.3,0.05,0.5,0.75,0.6,0.4] # we will use this as p in a bernouli distribution to get output of each arm\n",
    "num_bandits = len(bandits)\n",
    "weights = [1/num_bandits]*num_bandits\n",
    "epsilon = 0.5 # Random action with 50% probability\n",
    "num_episodes = 100\n",
    "test_episodes = num_episodes/10\n",
    "baseline = 0\n",
    "alpha = 0.8\n",
    "lr = 0.05\n",
    "running_reward = [0]*num_bandits\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we increase the number of bandits to 10.\n",
    "We expect bandit at index 2 should be the optimal choice but as we can see, the agent is picking up index 3\n",
    "Possible reasons - \n",
    "* Not enough episodes\n",
    "* Exploration is less compared to exploitation\n",
    "    * By chance the agent picked up index 3 and it kept on exploiting the choice \n",
    "To counter these we can try more episodes or increase epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Chosen arm: 3, Running rewards: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Iter: 10, Chosen arm: 3, Running rewards: [-1, -1, 0, 3, -1, 0, 1, 0, 0, -2]\n",
      "Iter: 20, Chosen arm: 3, Running rewards: [-1, -1, 0, 6, -1, -1, 0, 0, 1, -2]\n",
      "Iter: 30, Chosen arm: 0, Running rewards: [-3, -1, 0, 12, -1, -2, 1, 0, 1, -2]\n",
      "Iter: 40, Chosen arm: 3, Running rewards: [-3, -2, 0, 17, -1, -2, 0, 0, 1, -1]\n",
      "Iter: 50, Chosen arm: 3, Running rewards: [-2, -2, 0, 18, -3, -2, 1, 0, 1, -2]\n",
      "Iter: 60, Chosen arm: 3, Running rewards: [-2, -1, 0, 19, -4, -2, 0, 0, 1, -2]\n",
      "Iter: 70, Chosen arm: 8, Running rewards: [-3, -1, 1, 22, -5, -3, -2, 0, 2, -2]\n",
      "Iter: 80, Chosen arm: 3, Running rewards: [-3, -1, 2, 21, -5, -4, -3, 0, 2, -2]\n",
      "Iter: 90, Chosen arm: 3, Running rewards: [-3, -1, 2, 26, -5, -5, -3, 0, 4, -2]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    if np.random.uniform(low = 0.0, high = 1.0) < epsilon:\n",
    "        arm = np.random.randint(low = 0, high = len(bandits))\n",
    "    else:\n",
    "        arm = np.argmax(weights)\n",
    "    reward = np.random.binomial(1, bandits[arm])\n",
    "    if reward == 0:\n",
    "        reward = -1\n",
    "    running_reward[arm] += reward\n",
    "    advantage = reward - baseline\n",
    "    weights[arm] = weights[arm] + lr*advantage\n",
    "    baseline = baseline*alpha + (1 - alpha)*reward\n",
    "    if i%test_episodes == 0:\n",
    "        print(\"Iter: {}, Chosen arm: {}, Running rewards: {}\".format(i, arm, running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits = [0.1,0.4,0.9,0.7,0.3,0.05,0.5,0.75,0.6,0.4] # we will use this as p in a bernouli distribution to get output of each arm\n",
    "num_bandits = len(bandits)\n",
    "weights = [1/num_bandits]*num_bandits\n",
    "epsilon = 0.5 # Random action with 50% probability\n",
    "num_episodes = 1000\n",
    "test_episodes = num_episodes/10\n",
    "baseline = 0\n",
    "alpha = 0.8\n",
    "lr = 0.05\n",
    "running_reward = [0]*num_bandits\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of episodes did not really help. The agent picked up another sub optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Chosen arm: 0, Running rewards: [-1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Iter: 100, Chosen arm: 4, Running rewards: [-4, -2, 4, 2, -2, -4, -1, 23, 0, -3]\n",
      "Iter: 200, Chosen arm: 5, Running rewards: [-9, -3, 6, 5, -5, -12, -1, 50, 3, -5]\n",
      "Iter: 300, Chosen arm: 7, Running rewards: [-13, -5, 10, 11, -7, -12, -3, 73, 6, -5]\n",
      "Iter: 400, Chosen arm: 7, Running rewards: [-16, -5, 14, 15, -11, -18, -4, 76, 11, -7]\n",
      "Iter: 500, Chosen arm: 7, Running rewards: [-21, -7, 15, 18, -10, -24, -9, 104, 10, -7]\n",
      "Iter: 600, Chosen arm: 7, Running rewards: [-24, -6, 18, 17, -12, -30, -10, 130, 13, -15]\n",
      "Iter: 700, Chosen arm: 6, Running rewards: [-32, -4, 22, 20, -15, -37, -16, 156, 14, -17]\n",
      "Iter: 800, Chosen arm: 0, Running rewards: [-37, -7, 26, 23, -18, -45, -14, 180, 14, -17]\n",
      "Iter: 900, Chosen arm: 5, Running rewards: [-43, -6, 29, 23, -23, -55, -14, 210, 15, -17]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    if np.random.uniform(low = 0.0, high = 1.0) < epsilon:\n",
    "        arm = np.random.randint(low = 0, high = len(bandits))\n",
    "    else:\n",
    "        arm = np.argmax(weights)\n",
    "    reward = np.random.binomial(1, bandits[arm])\n",
    "    if reward == 0:\n",
    "        reward = -1\n",
    "    running_reward[arm] += reward\n",
    "    advantage = reward - baseline\n",
    "    weights[arm] = weights[arm] + lr*advantage\n",
    "    baseline = baseline*alpha + (1 - alpha)*reward\n",
    "    if i%test_episodes == 0:\n",
    "        print(\"Iter: {}, Chosen arm: {}, Running rewards: {}\".format(i, arm, running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandits = [0.1,0.4,0.9,0.7,0.3,0.05,0.5,0.75,0.6,0.4] # we will use this as p in a bernouli distribution to get output of each arm\n",
    "num_bandits = len(bandits)\n",
    "weights = [1/num_bandits]*num_bandits\n",
    "epsilon = 0.8 # Random action with 80% probability\n",
    "num_episodes = 1000\n",
    "test_episodes = num_episodes/10\n",
    "baseline = 0\n",
    "alpha = 0.8\n",
    "lr = 0.05\n",
    "running_reward = [0]*num_bandits\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing epsilon increased the exploration of the agent hence reaching the optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Chosen arm: 8, Running rewards: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Iter: 100, Chosen arm: 2, Running rewards: [-6, -3, 28, 2, 1, -6, 0, 4, 8, 1]\n",
      "Iter: 200, Chosen arm: 8, Running rewards: [-15, -2, 51, 2, -5, -11, 2, 9, 13, 1]\n",
      "Iter: 300, Chosen arm: 9, Running rewards: [-20, -9, 74, 3, -9, -18, 6, 12, 13, -3]\n",
      "Iter: 400, Chosen arm: 9, Running rewards: [-26, -10, 95, 7, -8, -26, 10, 15, 15, -5]\n",
      "Iter: 500, Chosen arm: 7, Running rewards: [-29, -11, 115, 6, -14, -31, 4, 22, 18, -11]\n",
      "Iter: 600, Chosen arm: 5, Running rewards: [-36, -12, 137, 10, -18, -44, 0, 27, 15, -16]\n",
      "Iter: 700, Chosen arm: 3, Running rewards: [-43, -18, 158, 12, -24, -52, 1, 32, 20, -19]\n",
      "Iter: 800, Chosen arm: 3, Running rewards: [-50, -24, 177, 13, -26, -63, -4, 36, 21, -15]\n",
      "Iter: 900, Chosen arm: 7, Running rewards: [-53, -24, 198, 12, -26, -73, -1, 46, 26, -18]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    if np.random.uniform(low = 0.0, high = 1.0) < epsilon:\n",
    "        arm = np.random.randint(low = 0, high = len(bandits))\n",
    "    else:\n",
    "        arm = np.argmax(weights)\n",
    "    reward = np.random.binomial(1, bandits[arm])\n",
    "    if reward == 0:\n",
    "        reward = -1\n",
    "    running_reward[arm] += reward\n",
    "    advantage = reward - baseline\n",
    "    weights[arm] = weights[arm] + lr*advantage\n",
    "    baseline = baseline*alpha + (1 - alpha)*reward\n",
    "    if i%test_episodes == 0:\n",
    "        print(\"Iter: {}, Chosen arm: {}, Running rewards: {}\".format(i, arm, running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
